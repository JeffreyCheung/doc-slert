<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
                 type="text/xml" 
                 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<article version="5.0" xml:id="art.slert.virtguide" xml:lang="en"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <?suse-quickstart color="suse"?>
  <title>Virtualization Guide</title>
  <subtitle>&productname; &productnumber;</subtitle>
  <info>
    <abstract>
      <para>It is possible to achieve isolation of RT workloads running
        alongside KVM by using standard methods, like cpusets, turning
        off irqbalance and routing IRQs to dedicated CPUs. Both libvirtd
        and KVM work fine in such configurations. </para>
      <para> None of the below observations and recommendations are
        specific to virtualization. Nevertheless, they can be considered
        best practices for isolating RT and KVM workloads, which are
        supported on a <quote>best-effort</quote> basis. </para>
    </abstract>
  </info>
  
  <sect1 xml:id="sec.virtguide.setup">
    <title>Setup</title>
    <para>All tests were carried out on a 48-core Xeon machine with 2
      NUMA nodes and 64GB of RAM running SLE12 RT and the 3.12.49-rt
      kernel. The virtual machine was installed with vm-install, running
      SLE12 SP1 on 4 CPUs and 2GB of memory. The disk used was physical
      disk <filename>/dev/sdb</filename> as recommended by the &suse;
      virtualization documentation.</para>
    <para>Two new cpuset directories were created, <filename>system</filename>
      and <filename>rt</filename>, and the 
      <systemitem>system</systemitem> task list was populated with:</para>
    <screen>for t in $(cat /sys/fs/cgroup/cpuset/tasks); do
    echo $t > /sys/fs/cgroup/cpuset/system/tasks
done</screen>
    <para>libvirtd automatically creates cpuset directories for each of
      the vCPUs along with a directory for the Qemu process. The CPU
      affinities in these directories were left unmodified and instead
      affinity was modified via the <command>virsh</command>
      <command>vcpupin</command> command.</para>
    <para>The CPUs were split into two groups: CPU 0-7 were allocated to
      the <systemitem>system</systemitem> and
        <systemitem>libvirtd</systemitem> cgroups and CPU 8-47 were
      allocated to the <systemitem>rt</systemitem> group. Having CPUs on
      the same socket in two groups was done intentionally to monitor
      the effects on shared CPU resources, such as LLC. </para>
    <para>The RT workload used throughout is
        <command>cyclictest</command>, executed via mmtests like so:
    </para>
    <screen>cyclictest -a 8-47 -t -n -m -p99 -d 0 -D 120 --quiet</screen>
  </sect1>
  
  <sect1 xml:id="sec.virtguide.observations">
    <title>Observations</title>
    <orderedlist>
      <listitem>
        <para>VM Heavy I/O</para>
        <para>The test for this was to do the following in a VM:</para>
        <screen>dd if=/dev/zero of=empty bs=4096 count=$(((80*1024*1024)/4096))</screen>
        <para>Doing large amounts of disk I/O in the VM guests has a
          noticeable impact on the latency of RT tasks. This is due to
          the constant eviction of LLC data, resulting in more cache
          misses.</para>
        <para>The maximum latencies in for the RT workload are seen on
          those CPUs on the same socket as the CPUs available to the KVM
          workload, for example, where the LLC is a shared resource between the
            <systemitem>system</systemitem> and
            <systemitem>rt</systemitem> cgroups.</para>
      </listitem>
      <listitem>
        <para>cpufreq drivers incur timer latency</para>
        <para>Drivers like intel_pstate will setup a timer on each CPU
          to periodically sample and adjust the CPU's current P-state.
          If this fires at an inopportune time it can add delays to the
          scheduling of RT tasks, particularly because lots of the
          IRQ/timer code paths run with interrupts disabled.</para>
      </listitem>
      <listitem>
        <para>Interrupt handling introduces delays</para>
        <para>The handling of interrupts can result in latencies that
          affect RT workloads. Interrupts should be routed to
          <quote>housekeeping</quote> CPUs that are not running RT
          applications.</para>
      </listitem>
      <listitem>
        <para>Some kernel threads cannot be controlled with cpuset</para>
        <para>Performing heavy I/O in the VM may cause kthreads to be
          scheduled on the CPUs dedicated for RT. This can occur, for
          example, when a kthread is flushing dirty pages to disk.
        </para>
        <para>While it is impossible to move some kworker threads into
          the <systemitem>system</systemitem> cgroup, the above issue can be mitigated by
          setting the CPU affinity for those threads via:</para>
        <screen>/sys/devices/virtual/workqueue/writeback/cpumask</screen>
      </listitem>
    </orderedlist>
  </sect1>
  
  <sect1 xml:id="sec.virtguide.rec">
    <title>Recommendations</title>
    <para>Suggestions for tuning machines running both RT and KVM
      workloads are as follows:</para>
    <orderedlist>
      <listitem>
        <para>Affinitize RT tasks to their own CPUs, and if possible to
          CPUs on their own dedicated socket. Using a dedicated socket
          avoids the issue from <xref
            linkend="sec.virtguide.observations"/> above where the LLC
          occupancy is churned by VMs doing lots of I/O operations. If
          that is not an option some customers may want to look at
          Intel's Cache Allocation Technology to further enforce cache
          allocation policies.</para>
      </listitem>
      <listitem>
        <para>Disable drivers that arm per-cpu timers such as cpufreq
          drivers, e.g. <code>intel_pstate=disable</code>.</para>
      </listitem>
      <listitem>
        <para>Set IRQ affinity to CPUs that are not running RT workloads
          and disable irqbalance.</para>
      </listitem>
      <listitem>
        <para>Search for cpumask control files in 
          <filename>/sys</filename> and set them
          appropriately for those cases that cannot be controlled via
          cpuset.</para>
      </listitem>
    </orderedlist>
  </sect1>
</article>
